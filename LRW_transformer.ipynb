{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Details for LRW dataset:\n",
    "- 500 words (check lrw_list.txt)\n",
    "- 800-1000 train videos per word\n",
    "- 50 test and 50 validation videos per word\n",
    "- Split into train, test, and validation sets\n",
    "- All videos are 29 frames long (1.16 seconds)\n",
    "- Word occurs roughly in the middle\n",
    "\n",
    "TODO: (update as completed)\n",
    "- Switch labels to numeric or one-hot (currently strings)\n",
    "- Experiement with transformations of the data\n",
    "- Experiement with trimming the video to cut out unneeded words, and reduce number of frames\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import pytorchvideo #look up how to use this\n",
    "import cv2\n",
    "import math\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange, repeat\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "from IPython.display import Video, HTML\n",
    "from playsound import playsound\n",
    "from tqdm import tqdm\n",
    "from timesformer_pytorch.rotary import apply_rot_emb, AxialRotaryEmbedding, RotaryEmbedding\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "NUM_FRAMES = 29 # Make sure to set all new videos to this length\n",
    "FPS = 25\n",
    "TIME = 1.16\n",
    "NUM_CLASSES = 50\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# device = 'cpu'\n",
    "random.seed(time.time())\n",
    "RANDOM_SEED = random.randint(0, 2**32 - 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LipReadDataset(Dataset):\n",
    "    def __init__(self, root_dir, split, device, transform=None):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "        Args:\n",
    "            video_paths (list): List of file paths to video files.\n",
    "            labels (list): List of corresponding labels.\n",
    "            transform (callable, optional): Optional data transformations (e.g., resizing, normalization).\n",
    "        \"\"\"\n",
    "        # self.word_folders = \n",
    "        # print(self.word_folders)\n",
    "        # self.frames = []\n",
    "        self.frame_titles = []\n",
    "        self.labels = []\n",
    "        self.device = device\n",
    "        self.word_dict = {}\n",
    "        self.num_classes = 0\n",
    "        self.transform = transform\n",
    "        # self.split = split\n",
    "        # self.prefix = root_dir\n",
    "        random.seed(RANDOM_SEED) #ensure words are the same bewtween datasets\n",
    "        word_folders = os.listdir(root_dir)\n",
    "        random.shuffle(word_folders)\n",
    "\n",
    "        for word_folder in word_folders:\n",
    "            set_path = os.path.join(root_dir, word_folder, split)\n",
    "            self.word_dict[word_folder] = self.num_classes\n",
    "            self.num_classes += 1\n",
    "            # want to test with 10 words first\n",
    "            if self.num_classes == NUM_CLASSES:\n",
    "                break\n",
    "            num_videos = 0\n",
    "            video_files = os.listdir(set_path)\n",
    "            random.seed(time.time()) #shuffle videos randomly\n",
    "            random.shuffle(video_files)\n",
    "            for video_file in video_files:\n",
    "                video_path = os.path.join(set_path, video_file)\n",
    "                if video_path.endswith('.txt'): # want to use later\n",
    "                    continue\n",
    "                num_videos += 1\n",
    "                if num_videos == 100:\n",
    "                    break\n",
    "                # curr_frames = self.extract_frames(video_path)\n",
    "                self.labels.append(word_folder)\n",
    "                # self.frames.append(curr_frames)\n",
    "                self.frame_titles.append(video_path)\n",
    "                print(f\"added {video_path} to {split} set\")\n",
    "        self.transform = transform\n",
    "        assert self.num_classes == len(self.word_dict)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the total number of samples in the dataset.\"\"\"\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a specific sample from the dataset.\n",
    "        Args:\n",
    "            idx (int): Index of the sample.\n",
    "        Returns:\n",
    "            video (tensor): Video frames (e.g., as a sequence of images).\n",
    "            label (int): Corresponding label.\n",
    "        \"\"\"\n",
    "        # extract video\n",
    "        # Apply any necessary transformations (e.g., resizing, normalization)\n",
    "        # if self.transform:\n",
    "        #     video = self.transform(video)\n",
    "        # Convert the label to a one-hot encoded tensor\n",
    "        return self.extract_frames(self.frame_titles[idx]), self.one_hot_encode(self.labels[idx])\n",
    "    \n",
    "    def one_hot_encode(self, labels):\n",
    "        # Convert labels to one-hot encoding\n",
    "        one_hot = torch.zeros(NUM_CLASSES, device=self.device)\n",
    "        one_hot[self.word_dict[labels]] = 1\n",
    "        return one_hot\n",
    "        \n",
    "\n",
    "    def extract_frames(self, video_path, duration=1.16, target_resolution = (40,65)):\n",
    "        # Read the video\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        # EXPERIEMENT WITH THIS\n",
    "        # duration_frames = int(duration * FPS)\n",
    "        # end_frame = NUM_FRAMES -  9 # skip the last 9 frames\n",
    "        # start_frame = end_frame - duration_frames # get middle frames\n",
    "        frames = []\n",
    "        # cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                # if cap.get(cv2.CAP_PROP_POS_FRAMES) == end_frame:\n",
    "                #     break\n",
    "                frame_cropped = frame[120:216, 80:176] # Crop the frame\n",
    "                # frame_resized = cv2.resize(frame_cropped, target_resolution)\n",
    "                if self.transform:\n",
    "                    frame_tensor = self.transform(frame_cropped)\n",
    "                # frame_gray = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2GRAY)\n",
    "                # frame_tensor = torch.from_numpy(frame_gray).to(self.device)\n",
    "                frame_tensor.to(self.device) #double check\n",
    "                frames.append(frame_tensor)\n",
    "            else:\n",
    "                break\n",
    "        cap.release()\n",
    "        video_tensor = torch.stack(frames, dim=0).to(self.device)\n",
    "        # video_tensor = video_tensor.permute(1, 0, 2, 3)  # assuming the original channel dimension is at index 1\n",
    "        return video_tensor\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Convert frames to PyTorch tensors\n",
    "    # transforms.Resize((40, 65)),  # Resize frames to smaller resolution\n",
    "    transforms.Grayscale(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5]),\n",
    "])\n",
    "\n",
    "\n",
    "root_dir = 'lipread_mp4'\n",
    "\n",
    "print(device)\n",
    "trainset = LipReadDataset(root_dir, split='train', device=device, transform=transform)\n",
    "print(\"finished trainset\")\n",
    "testset = LipReadDataset(root_dir, split='test',device=device, transform=transform)\n",
    "print(\"finished testset\")\n",
    "valset = LipReadDataset(root_dir, split='val', device=device, transform=transform)\n",
    "print(\"finished valset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#send to dataloader\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(valset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # TESTING IMAGE SIZE FOR CROP\n",
    "# video_path = trainset.frame_titles[2]\n",
    "\n",
    "# # def loop_video(video_path, loop_count, duration = 0.27):\n",
    "# #     # Read the video\n",
    "# #     cap = cv2.VideoCapture(video_path)\n",
    "# #     duration_frames = round(duration * FPS)\n",
    "# #     end_frame = NUM_FRAMES -  9 # skip the last 9 frames\n",
    "# #     start_frame = end_frame - duration_frames - 3  # get middle frames\n",
    "# #     print(end_frame, start_frame)\n",
    "# #     frames = []\n",
    "# #     cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "# #     # Loop the video\n",
    "# #     frame_count = 0\n",
    "# #     for _ in range(loop_count):\n",
    "# #         cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)  # Set frame position to the beginning\n",
    "# #         while cap.isOpened():\n",
    "# #             ret, frame = cap.read()\n",
    "# #             if ret:\n",
    "# #                 frame_count += 1\n",
    "# #                 if cap.get(cv2.CAP_PROP_POS_FRAMES) == end_frame:\n",
    "# #                     break\n",
    "# #                 frame_cropped = frame[120:200, 70:200]\n",
    "# #                 cv2.imshow('Looped Video', frame_cropped)\n",
    "# #                 if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "# #                     break\n",
    "# #             else:\n",
    "# #                 break\n",
    "# #     print(frame_count)\n",
    "    \n",
    "# #     # Release video capture\n",
    "# #     cap.release()\n",
    "# #     cv2.destroyAllWindows()\n",
    "\n",
    "# # loop_video(video_path, 10)\n",
    "# print(video_path)\n",
    "# def extract_frames( video_path, duration=1.16, target_resolution = (40,65), transform=None):\n",
    "#     # Read the video\n",
    "#     cap = cv2.VideoCapture(video_path)\n",
    "#     # EXPERIEMENT WITH THIS\n",
    "#     # duration_frames = int(duration * FPS)\n",
    "#     # end_frame = NUM_FRAMES -  9 # skip the last 9 frames\n",
    "#     # start_frame = end_frame - duration_frames # get middle frames\n",
    "#     frames = []\n",
    "#     # cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)\n",
    "\n",
    "#     while cap.isOpened():\n",
    "#         ret, frame = cap.read()\n",
    "#         if ret:\n",
    "#             # if cap.get(cv2.CAP_PROP_POS_FRAMES) == end_frame:\n",
    "#             #     break\n",
    "#             frame_cropped = frame[120:200, 70:200] # Crop the frame\n",
    "#             #downscale the image by half to 40x65\n",
    "#             # frame_cropped = cv2.resize(frame_cropped, target_resolution)\n",
    "#             # frame_resized = cv2.resize(frame_cropped, target_resolution)\n",
    "#             if transform:\n",
    "#                 frame_tensor = transform(frame_cropped)\n",
    "#             # frame_gray = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2GRAY)\n",
    "#             # frame_tensor = torch.from_numpy(frame_gray).to(self.device)\n",
    "#             frame_tensor = frame_tensor.to('cpu').numpy()\n",
    "#             frames.append(frame_tensor)\n",
    "#         else:\n",
    "#             break\n",
    "#     #show video for 10 loops\n",
    "#     loop_count = 10\n",
    "#     frame_count = 0\n",
    "#     cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "#     for i in range(loop_count):\n",
    "#         cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "#         for frame in frames:\n",
    "#             print(frame.shape)\n",
    "#             frame_count += 1\n",
    "#             cv2.imshow('Looped Video', frame[0])\n",
    "#             if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "#                 break\n",
    "#     cap.release()\n",
    "#     cv2.destroyAllWindows()\n",
    "#     # return video_tensor\n",
    "\n",
    "# extract_frames(video_path, transform=transform)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:21: SyntaxWarning: \"is\" with 'int' literal. Did you mean \"==\"?\n",
      "<>:21: SyntaxWarning: \"is\" with 'int' literal. Did you mean \"==\"?\n",
      "C:\\Users\\coler\\AppData\\Local\\Temp\\ipykernel_10656\\2177235015.py:21: SyntaxWarning: \"is\" with 'int' literal. Did you mean \"==\"?\n",
      "  if amt is 0:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# helpers\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "# classes\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        x = self.norm(x)\n",
    "        return self.fn(x, *args, **kwargs)\n",
    "\n",
    "# time token shift\n",
    "\n",
    "def shift(t, amt):\n",
    "    if amt is 0:\n",
    "        return t\n",
    "    return F.pad(t, (0, 0, 0, 0, amt, -amt))\n",
    "\n",
    "class PreTokenShift(nn.Module):\n",
    "    def __init__(self, frames, fn):\n",
    "        super().__init__()\n",
    "        self.frames = frames\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        f, dim = self.frames, x.shape[-1]\n",
    "        cls_x, x = x[:, :1], x[:, 1:]\n",
    "        x = rearrange(x, 'b (f n) d -> b f n d', f = f)\n",
    "\n",
    "        # shift along time frame before and after\n",
    "\n",
    "        dim_chunk = (dim // 3)\n",
    "        chunks = x.split(dim_chunk, dim = -1)\n",
    "        chunks_to_shift, rest = chunks[:3], chunks[3:]\n",
    "        shifted_chunks = tuple(map(lambda args: shift(*args), zip(chunks_to_shift, (-1, 0, 1))))\n",
    "        x = torch.cat((*shifted_chunks, *rest), dim = -1)\n",
    "\n",
    "        x = rearrange(x, 'b f n d -> b (f n) d')\n",
    "        x = torch.cat((cls_x, x), dim = 1)\n",
    "        return self.fn(x, *args, **kwargs)\n",
    "\n",
    "# feedforward\n",
    "\n",
    "class GEGLU(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x, gates = x.chunk(2, dim = -1)\n",
    "        return x * F.gelu(gates)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, mult = 4, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, dim * mult * 2),\n",
    "            GEGLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim * mult, dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# attention\n",
    "\n",
    "def attn(q, k, v, mask = None):\n",
    "    sim = einsum('b i d, b j d -> b i j', q, k)\n",
    "\n",
    "    if exists(mask):\n",
    "        max_neg_value = -torch.finfo(sim.dtype).max\n",
    "        sim.masked_fill_(~mask, max_neg_value)\n",
    "\n",
    "    attn = sim.softmax(dim = -1)\n",
    "    out = einsum('b i j, b j d -> b i d', attn, v)\n",
    "    return out\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        dim_head = 64,\n",
    "        heads = 8,\n",
    "        dropout = 0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "        inner_dim = dim_head * heads\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, einops_from, einops_to, mask = None, cls_mask = None, rot_emb = None, **einops_dims):\n",
    "        h = self.heads\n",
    "        q, k, v = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> (b h) n d', h = h), (q, k, v))\n",
    "\n",
    "        q = q * self.scale\n",
    "\n",
    "        # splice out classification token at index 1\n",
    "        (cls_q, q_), (cls_k, k_), (cls_v, v_) = map(lambda t: (t[:, :1], t[:, 1:]), (q, k, v))\n",
    "\n",
    "        # let classification token attend to key / values of all patches across time and space\n",
    "        cls_out = attn(cls_q, k, v, mask = cls_mask)\n",
    "\n",
    "        # rearrange across time or space\n",
    "        q_, k_, v_ = map(lambda t: rearrange(t, f'{einops_from} -> {einops_to}', **einops_dims), (q_, k_, v_))\n",
    "\n",
    "        # add rotary embeddings, if applicable\n",
    "        if exists(rot_emb):\n",
    "            q_, k_ = apply_rot_emb(q_, k_, rot_emb)\n",
    "\n",
    "        # expand cls token keys and values across time or space and concat\n",
    "        r = q_.shape[0] // cls_k.shape[0]\n",
    "        cls_k, cls_v = map(lambda t: repeat(t, 'b () d -> (b r) () d', r = r), (cls_k, cls_v))\n",
    "\n",
    "        k_ = torch.cat((cls_k, k_), dim = 1)\n",
    "        v_ = torch.cat((cls_v, v_), dim = 1)\n",
    "\n",
    "        # attention\n",
    "        out = attn(q_, k_, v_, mask = mask)\n",
    "\n",
    "        # merge back time or space\n",
    "        out = rearrange(out, f'{einops_to} -> {einops_from}', **einops_dims)\n",
    "\n",
    "        # concat back the cls token\n",
    "        out = torch.cat((cls_out, out), dim = 1)\n",
    "\n",
    "        # merge back the heads\n",
    "        out = rearrange(out, '(b h) n d -> b n (h d)', h = h)\n",
    "\n",
    "        # combine heads out\n",
    "        return self.to_out(out)\n",
    "\n",
    "# main classes\n",
    "\n",
    "class TimeSformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        num_frames,\n",
    "        num_classes,\n",
    "        image_size = 96,\n",
    "        patch_size = 16,\n",
    "        channels = 1,\n",
    "        depth = 12,\n",
    "        heads = 8,\n",
    "        dim_head = 64,\n",
    "        attn_dropout = 0.,\n",
    "        ff_dropout = 0.,\n",
    "        rotary_emb = True,\n",
    "        shift_tokens = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert image_size % patch_size == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "\n",
    "        num_patches = (image_size // patch_size) ** 2\n",
    "        num_positions = num_frames * num_patches\n",
    "        patch_dim = channels * patch_size ** 2\n",
    "\n",
    "        self.heads = heads\n",
    "        self.patch_size = patch_size\n",
    "        self.to_patch_embedding = nn.Linear(patch_dim, dim)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, dim))\n",
    "\n",
    "        self.use_rotary_emb = rotary_emb\n",
    "        if rotary_emb:\n",
    "            self.frame_rot_emb = RotaryEmbedding(dim_head)\n",
    "            self.image_rot_emb = AxialRotaryEmbedding(dim_head)\n",
    "        else:\n",
    "            self.pos_emb = nn.Embedding(num_positions + 1, dim)\n",
    "\n",
    "\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            ff = FeedForward(dim, dropout = ff_dropout)\n",
    "            time_attn = Attention(dim, dim_head = dim_head, heads = heads, dropout = attn_dropout)\n",
    "            spatial_attn = Attention(dim, dim_head = dim_head, heads = heads, dropout = attn_dropout)\n",
    "\n",
    "            if shift_tokens:\n",
    "                time_attn, spatial_attn, ff = map(lambda t: PreTokenShift(num_frames, t), (time_attn, spatial_attn, ff))\n",
    "\n",
    "            time_attn, spatial_attn, ff = map(lambda t: PreNorm(dim, t), (time_attn, spatial_attn, ff))\n",
    "\n",
    "            self.layers.append(nn.ModuleList([time_attn, spatial_attn, ff]))\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, video, mask = None):\n",
    "        b, f, _, h, w, *_, device, p = *video.shape, video.device, self.patch_size\n",
    "        assert h % p == 0 and w % p == 0, f'height {h} and width {w} of video must be divisible by the patch size {p}'\n",
    "\n",
    "        # calculate num patches in height and width dimension, and number of total patches (n)\n",
    "\n",
    "        hp, wp = (h // p), (w // p)\n",
    "        n = hp * wp\n",
    "\n",
    "        # video to patch embeddings\n",
    "        video = rearrange(video, 'b f c (h p1) (w p2) -> b (f h w) (p1 p2 c)', p1 = p, p2 = p)\n",
    "\n",
    "        tokens = self.to_patch_embedding(video)\n",
    "\n",
    "        # add cls token\n",
    "\n",
    "        cls_token = repeat(self.cls_token, 'n d -> b n d', b = b)\n",
    "        x =  torch.cat((cls_token, tokens), dim = 1)\n",
    "\n",
    "        # positional embedding\n",
    "\n",
    "        frame_pos_emb = None\n",
    "        image_pos_emb = None\n",
    "        if not self.use_rotary_emb:\n",
    "            x += self.pos_emb(torch.arange(x.shape[1], device = device))\n",
    "        else:\n",
    "            frame_pos_emb = self.frame_rot_emb(f, device = device)\n",
    "            image_pos_emb = self.image_rot_emb(hp, wp, device = device)\n",
    "\n",
    "        # calculate masking for uneven number of frames\n",
    "\n",
    "        frame_mask = None\n",
    "        cls_attn_mask = None\n",
    "        if exists(mask):\n",
    "            mask_with_cls = F.pad(mask, (1, 0), value = True)\n",
    "\n",
    "            frame_mask = repeat(mask_with_cls, 'b f -> (b h n) () f', n = n, h = self.heads)\n",
    "\n",
    "            cls_attn_mask = repeat(mask, 'b f -> (b h) () (f n)', n = n, h = self.heads)\n",
    "            cls_attn_mask = F.pad(cls_attn_mask, (1, 0), value = True)\n",
    "\n",
    "        # time and space attention\n",
    "\n",
    "        for (time_attn, spatial_attn, ff) in self.layers:\n",
    "            x = time_attn(x, 'b (f n) d', '(b n) f d', n = n, mask = frame_mask, cls_mask = cls_attn_mask, rot_emb = frame_pos_emb) + x\n",
    "            x = spatial_attn(x, 'b (f n) d', '(b f) n d', f = f, cls_mask = cls_attn_mask, rot_emb = image_pos_emb) + x\n",
    "            x = ff(x) + x\n",
    "\n",
    "        cls_token = x[:, 0]\n",
    "        return self.to_out(cls_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(loss, acc, title):\n",
    "    fig, ax = plt.subplots(2)\n",
    "    fig.tight_layout(pad=3.0)\n",
    "    fig.suptitle(title)\n",
    "    ax[0].plot(loss)\n",
    "    ax[0].set_title('Training Loss')\n",
    "    ax[0].set_xlabel('Epoch')\n",
    "    ax[0].set_ylabel('Loss')\n",
    "    ax[1].plot(acc)\n",
    "    ax[1].set_title('Training Accuracy')\n",
    "    ax[1].set_xlabel('Epoch')\n",
    "    ax[1].set_ylabel('Accuracy')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_val(model, train_loader, val_loader, optimizer, criterion, num_epochs=10):\n",
    "    model.train()\n",
    "    train_loss_epoch = []\n",
    "    train_acc_epoch = []\n",
    "    val_loss_epoch = []\n",
    "    val_acc_epoch = []\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0.0\n",
    "        train_total = 0\n",
    "        train_correct = 0\n",
    "        val_loss = 0.0\n",
    "        val_total = 0\n",
    "        val_correct = 0\n",
    "        for data in tqdm(train_loader):\n",
    "            inputs, labels = data\n",
    "            inputs.to(device)\n",
    "            labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            _, labels_class = labels.max(dim=1)  \n",
    "            _, predicted_class = outputs.max(dim=1)  \n",
    "            train_correct += (predicted_class == labels_class).sum().item()\n",
    "            \n",
    "            train_total += labels.size(0)\n",
    "            #print\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "\n",
    "        #save model with name based on time\n",
    "        epoch_state_dict = {'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}\n",
    "        torch.save(epoch_state_dict, f'transformer_{int(time.time())}.pth')\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for data in tqdm(val_loader):\n",
    "                inputs, labels = data\n",
    "                inputs.to(device)\n",
    "                labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, labels_class = labels.max(dim=1)  \n",
    "                _, predicted_class = outputs.max(dim=1)  \n",
    "                val_correct += (predicted_class == labels_class).sum().item()\n",
    "                val_total+= labels.size(0)\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f'Epoch {epoch + 1}\\nTrain Loss: {train_loss }, Accuracy: {100 * train_correct / train_total}% \\nVal Loss: {val_loss}, Accuracy: {100 * val_correct / val_total}%')\n",
    "        train_loss_epoch.append(train_loss)\n",
    "        train_acc_epoch.append(100 * train_correct / train_total)\n",
    "        val_loss_epoch.append(val_loss)\n",
    "        val_acc_epoch.append(100 * val_correct / val_total)\n",
    "    plot(train_loss_epoch, train_acc_epoch, 'Training')\n",
    "    plot(val_loss_epoch, val_acc_epoch, 'Validation')\n",
    "\n",
    "    print('Finished Training')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            inputs, labels = datac\n",
    "            inputs.to(device)\n",
    "            labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            # print(labels.shape, outputs.shape)\n",
    "            _, labels_class = labels.max(dim=1)  \n",
    "            _, predicted_class = outputs.max(dim=1)  \n",
    "            test_correct += (predicted_class == labels_class).sum().item()\n",
    "            # print(predicted_class, labels_class, correct)\n",
    "            # accuracy = correct_predictions / labels.size(0)  \n",
    "            test_total += labels.size(0)\n",
    "    print(f'Accuracy: {100 * test_correct / test_total}%')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 82/607 [10:09:59<1507:40:23, 10338.33s/it]"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "#more complex\n",
    "model = TimeSformer(\n",
    "    dim = 512,\n",
    "    num_frames = NUM_FRAMES,\n",
    "    num_classes = NUM_CLASSES,\n",
    "    image_size = 96,\n",
    "    patch_size = 8,\n",
    "    depth = 12,\n",
    "    heads = 8,\n",
    "    dim_head = 32,\n",
    "    attn_dropout = 0.1,\n",
    "    ff_dropout = 0.1,\n",
    "    rotary_emb = True,\n",
    "    shift_tokens = False\n",
    ").to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "train_and_val(model, train_loader, val_loader, optimizer, criterion, num_epochs=10)\n",
    "test(model, test_loader)\n",
    "\n",
    "cnn2_state_dict = {'model_state_dict': model.state_dict(), 'optimizer_state_dict': optimizer.state_dict()}\n",
    "torch.save(cnn2_state_dict, 'transformer.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ec523",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
