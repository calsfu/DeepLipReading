{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 214,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNjGoCv-DCQo",
        "outputId": "a5ae5c1c-007e-42f9-a7bb-b222431eeed3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ten1LxtvGuqo"
      },
      "outputs": [],
      "source": [
        "# !unzip /content/drive/MyDrive/Senior/Spring/EC523/FinalProject/ec523/data.zip\n",
        "!unzip /content/drive/MyDrive/School/Graduate/Classes/2024\\ Spring/EC523\\ Deep\\ Learning/data.zip\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mb9zo7OTR_L"
      },
      "source": [
        "# Performing lip detection and extraction from a video using a Haar cascade classifier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def process_video_file(path: str) -> tf.Tensor:\n",
        "    \"\"\"\n",
        "    Images are now fully pre-processed here before being stored or saved.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load video\n",
        "    cap = cv2.VideoCapture(path)  # e.g. /content/data/videos/bbaf2n.mpg\n",
        "\n",
        "    frame_count = 0\n",
        "    # processed_frames = []\n",
        "    tensors_list = []\n",
        "\n",
        "    while True:\n",
        "\n",
        "        # Read frame from vid\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Convert to grey scale\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Lip detection\n",
        "        for x in reversed(range(5, 8)):  # lower conditions as needed to find best match\n",
        "            lips = lip_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=x, minSize=(60, 30), maxSize=(120, 60))\n",
        "            if len(lips) > 0:\n",
        "                break\n",
        "        if len(lips) == 0:  # if still no match, use static vals\n",
        "            lips = [[50, 150, 200, 100]]\n",
        "        (x, y, w, h) = lips[0]\n",
        "        # print(f'{frame_count:02d}', ': ', lips[0])\n",
        "\n",
        "        # Store normalized, grayscaled, and uniformly cropped\n",
        "        gray_normal_crop = cv2.normalize(gray, gray, 0, 100, cv2.NORM_MINMAX)[y:y+h, x:x+w]\n",
        "        final_img = cv2.resize(gray_normal_crop, (80, 40))\n",
        "        tensors_list.append(final_img)\n",
        "\n",
        "        # Stored in tensor, rather than saving to file\n",
        "        # cv2.imwrite(os.path.join(output_dir, f'frame_{frame_count}.jpg'), final_img)\n",
        "        # cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 200, 0), 2)  # draws rectangle\n",
        "        # cv2.imwrite(os.path.join(output_dir, f'frame_{frame_count}.jpg'), frame)\n",
        "        # processed_frames.append(cv2.normalize(gray, gray, 0, 100, cv2.NORM_MINMAX)[y:y+h, x:x+w])\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "    # TODO: tf.normalize across all images\n",
        "    # mean = tf.math.reduce_mean(processed_frames)\n",
        "    # std = tf.math.reduce_std(tf.cast(processed_frames, tf.float32))\n",
        "    # processed_frames = (tf.cast((processed_frames - mean), tf.float32) / std).numpy()\n",
        "    # for i, v in enumerate(processed_frames):\n",
        "    #     output_path = os.path.join(output_dir, f\"frame_{i}.jpg\")\n",
        "    #     cv2.imwrite(output_path, v)\n",
        "    # print(int(cap.get(cv2.CAP_PROP_FRAME_COUNT)))  # number of frames in video\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    video_tensor = tf.convert_to_tensor(np.array(tensors_list))\n",
        "\n",
        "    # print(f\"Finished processing video; {frame_count} frames saved in:\", output_dir)\n",
        "    # print(f'\\nvideo_tensor ({type(video_tensor)}, {video_tensor.shape}): \\n', video_tensor)\n",
        "\n",
        "    return video_tensor\n",
        "\n",
        "\n",
        "# Load annotations\n",
        "def load_annotations(annotation_file: str) -> list[str]:\n",
        "\n",
        "    with open(annotation_file, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "        annotations = [line.strip().split() for line in lines]\n",
        "\n",
        "    return annotations\n",
        "\n",
        "\n",
        "def process_annotations_file(path: str):\n",
        "    \"\"\"\n",
        "    Should experiment with syllable labels vs. video labels...\n",
        "    \"\"\"\n",
        "\n",
        "    # annotation_file = \"/content/data/annotations/s1/bbaf2n.align\"\n",
        "    annotations = load_annotations(path)\n",
        "\n",
        "    # Map frames to annotations based on filename; each annotation line contains:\n",
        "    # [<start of syllable to thousandth of a frame>,\n",
        "    #    <end of syllable to thousandth of a frame>,\n",
        "    #    <syllable/annotation>]\n",
        "    # frame_labels = []\n",
        "    video_label = []\n",
        "    for line in annotations:\n",
        "        # rng = np.array([int(line[0]), int(line[1])]) // 1000\n",
        "        # frame_labels[rng[0]:rng[1] + 1] = [line[2]] * ((rng[1] + 1) - rng[0])\n",
        "        for letter in line[2]:\n",
        "            video_label.append(ord(letter) - 97)\n",
        "    # print(f'\\nannotated_frames ({(cnt := len(frame_labels))}): \\n', frame_labels)\n",
        "    # print(f'\\nvideo_label ({len(video_label)}): \\n', video_label)\n",
        "\n",
        "    # Tests (for bbaf2n)\n",
        "    # assert cnt == frame_count\n",
        "    # assert frame_labels[33] == 'blue' and frame_labels[34]      == 'at'\n",
        "    # assert frame_labels[0]  == 'sil'  and frame_labels[cnt - 1] == 'sil'\n",
        "    # assert video_label[0]   == 18     and video_label[3]        == 1\n",
        "\n",
        "    # TODO: Remove 'sil' (i.e. silence) frames?\n",
        "\n",
        "    return video_label\n"
      ],
      "metadata": {
        "id": "kd6eHSPEO8bX"
      },
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 242,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "CCkxP9xRI1gF",
        "outputId": "40b58385-0673-4d95-eb78-11f7bad9b1f8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (204,) + inhomogeneous part.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-242-d63c426771e5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (204,) + inhomogeneous part."
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "\n",
        "# Directory to store processed frames\n",
        "output_dir = \"/content/processed_frames\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "videos_dir = '/content/data/videos'\n",
        "annotations_dir = '/content/data/annotations/s1'\n",
        "video_limit = 100  # On Google Colab Pro, 100 takes about ~6 mins to load\n",
        "\n",
        "# Use Haar cascade for lip detection\n",
        "lip_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_smile.xml')\n",
        "\n",
        "# Loop over files, loading the\n",
        "# video_subset = ['bbaf2n', 'bbaf3s', 'bbaf4p']\n",
        "inputs = []\n",
        "labels = []\n",
        "\n",
        "for i, video_name in enumerate(os.listdir(videos_dir)):\n",
        "    if video_name.endswith('.mpg'):\n",
        "        inputs.append(process_video_file(f'{videos_dir}/{video_name}'))\n",
        "        print(i, ': ', video_name)\n",
        "    if i > video_limit:\n",
        "        break\n",
        "\n",
        "for i, annotations_name in enumerate(os.listdir(annotations_dir)):\n",
        "    if annotations_name.endswith('.align'):\n",
        "        labels.append(process_annotations_file(f'{annotations_dir}/{annotations_name}'))\n",
        "        print(i, ': ', annotations_name)\n",
        "    if i > video_limit:\n",
        "        break\n",
        "\n",
        "inputs = np.array([inputs])\n",
        "labels = np.array([labels])\n",
        "\n",
        "print(f'\\ninputs ({inputs.shape}): \\n')#, inputs)\n",
        "print(f'\\nlabels ({labels.shape}): \\n')#, labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {
        "id": "gphyF2_nQKqx"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Now being handled above\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers, models\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Define width, height, and number of frames\n",
        "width      = 80  # originally 224\n",
        "height     = 40  # originally 224\n",
        "num_frames = 75  # Number of frames per video (currently using all frames)\n",
        "channels   = 3\n",
        "\n",
        "\n",
        "# extract label from filename\n",
        "def extract_label(filename):\n",
        "    label = filename.split(\"_\")[1].split(\".\")[0]  # labels are now defined above\n",
        "    return label\n",
        "\n",
        "\n",
        "# Function to load and preprocess frames\n",
        "# (Now being handled above, before the images are saved, to reduce space)\n",
        "def load_and_preprocess_frames(directory):\n",
        "    frames = []\n",
        "    labels = []\n",
        "\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".jpg\"):\n",
        "            # load the frame\n",
        "            img = cv2.imread(os.path.join(directory, filename))\n",
        "            # preprocess frame (resize, normalize, etc.)\n",
        "            img = cv2.resize(img, (width, height))  # resize frame\n",
        "            img = img / 255.0  # normalize pixel values to [0, 1]\n",
        "            # append the frame and label\n",
        "            frames.append(img)\n",
        "            labels.append(extract_label(filename))  # extract label\n",
        "\n",
        "    return np.array(labels)#, np.array(frames)\n",
        "\n",
        "\n",
        "# Load and preprocess frames\n",
        "# labels = load_and_preprocess_frames(\"/content/processed_frames\")\n",
        "# print(labels)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9EuUIInQcWp"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# split the data into training and validation sets\n",
        "# X_train, X_val, y_train, y_val = train_test_split(frames, labels, test_size=0.2, random_state=42)\n",
        "# print(f'\\nX_train ({X_train.shape}): \\n', X_train)\n",
        "# print(f'\\nX_val   ({X_val.shape}):   \\n', X_val)\n",
        "# print(f'\\ny_train ({y_train.shape}): \\n', y_train)\n",
        "# print(f'\\ny_val   ({y_val.shape}):   \\n', y_val)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-Z2rvDEUIwN"
      },
      "source": [
        "# Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "1KJV4mMuQhvZ",
        "outputId": "29b6a751-4530-488f-932b-a1494c2fbb71"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "The `kernel_size` argument must be a tuple of 2 integers. Received: (3, 3, 3)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-df61da0e0241>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m model = models.Sequential([\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# 3D Convolutional layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'same'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaxPooling3D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'same'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/dtensor/utils.py\u001b[0m in \u001b[0;36m_wrap_function\u001b[0;34m(layer_instance, *args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m                     \u001b[0mlayout_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvariable_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_layout\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0minit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_instance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;31m# Inject the layout parameter after the invocation of __init__()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/conv2d.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filters, kernel_size, strides, padding, data_format, dilation_rate, groups, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     ):\n\u001b[0;32m--> 179\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0mrank\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, rank, filters, kernel_size, strides, padding, data_format, dilation_rate, groups, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, trainable, name, conv_op, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroups\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m         self.kernel_size = conv_utils.normalize_tuple(\n\u001b[0m\u001b[1;32m    137\u001b[0m             \u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"kernel_size\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/conv_utils.py\u001b[0m in \u001b[0;36mnormalize_tuple\u001b[0;34m(value, n, name, allow_zero)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue_tuple\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msingle_value\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalue_tuple\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The `kernel_size` argument must be a tuple of 2 integers. Received: (3, 3, 3)"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers, models\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Assuming 'height', 'width', 'frames', and 'channels' are defined\n",
        "# Calculate the number of unique classes or labels\n",
        "num_classes = len(set(labels))\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train_encoded = to_categorical(y_train, num_classes)\n",
        "y_val_encoded   = to_categorical(y_val,   num_classes)\n",
        "\n",
        "# Define the lip reading model architecture\n",
        "model = models.Sequential([\n",
        "    # 3D Convolutional layers\n",
        "    layers.Conv3D(32,   (3, 3, 3), activation='relu', padding='same', input_shape=(num_frames, height, width, channels)),\n",
        "    layers.MaxPooling3D((2, 2, 2)),\n",
        "    layers.Conv3D(64,   (3, 3, 3), activation='relu', padding='same'),\n",
        "    layers.MaxPooling3D((2, 2, 2)),\n",
        "    layers.Conv3D(128,  (3, 3, 3), activation='relu', padding='same'),\n",
        "    layers.MaxPooling3D((1, 2, 2)),  # Adjusted pooling size\n",
        "\n",
        "    # Flatten layer\n",
        "    layers.Flatten(),\n",
        "\n",
        "    # Fully connected layers\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_Mhxx7UUMOq"
      },
      "source": [
        "# Model Accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 809
        },
        "id": "qJYtK0CCRl-z",
        "outputId": "1e41b0d5-1166-4d36-d712-02c0f0cd3e15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_12\" is incompatible with the layer: expected shape=(None, 10, 224, 224, 3), found shape=(None, 224, 224, 3)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-92e46173b501>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# train model using your processed frames as input data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_encoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val_encoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# model performance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1150, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_12\" is incompatible with the layer: expected shape=(None, 10, 224, 224, 3), found shape=(None, 224, 224, 3)\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# train model using your processed frames as input data\n",
        "history = model.fit(X_train, y_train_encoded, epochs=10, batch_size=32, validation_data=(X_val, y_val_encoded))\n",
        "\n",
        "# model performance\n",
        "loss, accuracy = model.evaluate(X_val, y_val_encoded)\n",
        "print(\"Validation Loss:\", loss)\n",
        "print(\"Validation Accuracy:\", accuracy)\n",
        "\n",
        "# plot training history\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}